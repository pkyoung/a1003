{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c90eb36-7847-44dd-b118-6bb46ccc498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "raw_text = \"\"\"The sky turned red.\n",
    "She ran toward the tower.\n",
    "No one answered the radio.\n",
    "“Is anybody out there?” she whispered.\n",
    "Static hissed back in reply.\n",
    "Her flashlight flickered, then died.\n",
    "Footsteps echoed behind her.\n",
    "She turned, heart pounding.\n",
    "A voice said, “You made it.”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfee4a6-0289-4c63-8797-800d9b99f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.tokenize.sent_tokenize(raw_text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b77655-0a08-47e5-bbdd-bb41f3ad08f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.tokenize.sent_tokenize(raw_text.replace('\\n', ' '))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81115520-52e6-46fa-a16c-3192554af9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    # 단어 토큰화\n",
    "    tokenized_sentence = nltk.tokenize.word_tokenize(sentence)\n",
    "    result = []\n",
    "\n",
    "    for word in tokenized_sentence: \n",
    "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄인다.\n",
    "        result.append(word)\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 0 \n",
    "        vocab[word] += 1\n",
    "    preprocessed_sentences.append(result) \n",
    "print(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e82ac-16a8-4080-9e72-83beb8579784",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82058c85-3a3b-4775-8018-2f6f92bea218",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4580e0d7-fe87-438b-be19-e9a4f9aae76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "index = 2\n",
    "cut_off = 0\n",
    "for word in vocab :\n",
    "    frequency = vocab[word]\n",
    "    if frequency > cut_off:\n",
    "        word_to_index[word] = index\n",
    "        index = index + 1\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818bf0e5-6814-43f2-9acc-85657adcfb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sentences = []\n",
    "for sentence in preprocessed_sentences:\n",
    "    encoded_sentence = []\n",
    "    for word in sentence:\n",
    "        encoded_sentence.append(word_to_index[word])\n",
    "    encoded_sentences.append(encoded_sentence)\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0981eff1-a27b-4885-9491-89b3849292fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"Then the signal went dark.\"\n",
    "tokenized_sentence = nltk.tokenize.word_tokenize(new_text)\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e1fe04-930c-4552-8561-f2296c08efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sentence = []\n",
    "for word in tokenized_sentence:\n",
    "    word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄인다.\n",
    "    encoded_sentence.append(word_to_index[word])\n",
    "print(encoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ddc5a-d7b6-44fb-a5c1-d6a6345552a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sentence = []\n",
    "word_to_index['<pad>'] = 0\n",
    "word_to_index['<unk>'] = 1\n",
    "for word in tokenized_sentence:\n",
    "    try:\n",
    "        word = word.lower()\n",
    "        encoded_sentence.append(word_to_index[word])\n",
    "    except KeyError:\n",
    "        # 만약 단어 집합에 없는 단어라면 'OOV'의 정수를 리턴.\n",
    "        encoded_sentence.append(word_to_index['<unk>'])\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755433ed-9983-4856-9e3c-99c0ca0a66b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a184ed-71e2-4572-9ec7-cd239a7fe7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(s) for s in encoded_sentences)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91baf87b-ce6c-44b5-8f81-8e5e6944f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in encoded_sentences:\n",
    "    while len(sentence) < max_len:\n",
    "        sentence.append(word_to_index['<pad>'])\n",
    "\n",
    "encoded_sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
